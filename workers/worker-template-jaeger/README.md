# Consumer Skeleton Project

## Table of Contents

1. [Introduction](#introduction)
2. [Scope of the Project](#scope-of-the-project)
3. [Project Structure](#project-structure)
4. [Architecture Overview](#architecture-overview)
5. [Database](#database)
6. [Setup and Installation](#setup-and-installation)
7. [Usage](#usage)
8. [Customization and Implementation](#customization-and-implementation)
9. [Considerations](#considerations)
10. [Future Enhancements](#future-enhancements)

## Introduction

This project provides a template for building Kafka workers/on demand API calls that read messages from one or more  Kafka topics/HTTP requests,
aggregate them based on a common identifier (e.g., `id`), process the aggregated message, and then forward the processed
message to one or more output Kafka topics/as HTTP responses. This skeleton is designed to be easily extendable and  customizable, allowing
developers to implement specific processing logic by modifying a single function.

## Scope of the Project

The scope of this project is to provide a template or boilerplate for **QSINT workers** that require
message aggregation and custom processing logic. The template is designed to be extendable and flexible, allowing
developers to easily implement their own processing logic while handling common tasks like message consumption, aggregation, and production.

## Project Structure

```plaintext
.
├── framework
│   └── api
│   └── auth
│   └── common
│   └── etl
│       └── framework_etl.py
│   └── instances
│   └── streams
├── ops_service
│   └── worker_api.py
├── models
│   └── instances.py
│   └── models.py
├── maps
│   └── endpoint.json
│   └── realm.json
├── tls
│   └── wildcard.cluster.local.crt
│   └── wildcard.cluster.local.key
├── worker.py             # Contains the process() function to customize processing logic
├── main.py               # Entry point for running the Kafka consumer
├── models.py             # ORM models and database initialization
├── config.py             # Configuration settings for the database and consumer
└── README.md             # Project documentation (this file)
```

# Architecture Overview

![Architecture](./images/qsint-core-consumers-kafka.drawio.png)

The architecture of the project is designed to be modular, allowing for easy customization and scaling. The key
components are:

1. **Kafka Consumer**: Reads messages from one or more input topics.
2. **Kafka Producer**: Sends processed messages to one or more output topics.
3. **Autogenerated API**: Auto-generated API (exposed via _Swagger_) based on `maps/endpoint.json`
4. **Database Configuration**: Fetches consumer configuration from a database, making the consumer highly configurable.
5. **Message Aggregation**: Aggregates messages from different topics based on a common id.
6. **Processing Logic**: Custom logic implemented in the process() function in `worker_api.py` and `worker_kafka`.
7. **Timeout Handling**: Ensures that incomplete message aggregations are discarded after a configurable timeout.

## Database

The consumer configuration is stored in a database table (`consumer_configs`). The `CONSUMER_NAME` in `config.py`
determines which configuration to load. The configuration includes:

- **Input Topics**: Topics from which the consumer will read messages.
- **Output Topics**: Topics to which the processed messages will be sent.
- **Kafka Bootstrap Servers**: Kafka brokers to connect to.
- **Timeout**: Timeout in seconds for message aggregation before discarding incomplete messages.

### Database Table Structure

| id | consumer_name | topics_input    | topics_output   | metadatas | kafka_bootstrap_server | timeout |
|----|---------------|-----------------|-----------------|-----------|------------------------|---------|
| 1  | consumer1     | topic_1         | topic_2         | <null>    | 172.17.12.80:9092      | 30      |
| 2  | consumer2     | topic_2         | topic_3,topic_4 | <null>    | 172.17.12.80:9092      | 60      |
| 3  | consumer3     | topic_3         | topic_5         | <null>    | 172.17.12.80:9092      | 45      |
| 4  | consumer4     | topic_4         | topic_6         | <null>    | 172.17.12.80:9092      | 30      |
| 5  | consumer5     | topic_5,topic_6 | topic_7         | <null>    | 172.17.12.80:9092      | 30      |

- **`consumer_name`**: A unique identifier for the consumer. This should match the `CONSUMER_NAME` defined in
  `config.py`.
- **`topics_input`**: A comma-separated list of Kafka topics that the consumer will read from.
- **`topics_output`**: A comma-separated list of Kafka topics to which the processed messages will be sent.
- **`metadatas`**: Additional metadata for the consumer (optional).
- **`kafka_bootstrap_server`**: The Kafka bootstrap server the consumer will connect to.
- **`timeout`**: The timeout in seconds for message aggregation before discarding incomplete messages.

## Setup and Installation

#### Prerequisites

- Python 3.10+
- MySQL database (or any other SQL database with minor modifications in `models.py`)
- Kafka broker

#### Installation

1. Set up a virtual environment:

```code
python3 -m venv venv
source venv/bin/activate
```

2. Install dependencies

```code
pip install -r requirements.txt
```

3. Configure database (**MySQL**) and Consumer Name

Update the `.env` file with your database credentials, worker name, jaeger, etc:

```code
# ====================================================
# SPECIFICS-FRAMEWORK-UTILS:
# ====================================================
# disable role checking (ONLY FOR DEV)
DISABLE_SECURITY=True

# GENERICS
# ----------------------------------------------------
FLASK_APP=app.py
FLASK_DEBUG=development
# ----------------------------------------------------

# ====================================================
# DATABASE:
# ----------------------------------------------------
DB_HOST='172.17.120.23'
DB_PORT=3306
DB_NAME='dev'
DB_USER='dev'
DB_PASSWORD='dev'
DB_TABLE_NAME='consumer_configs'

# ====================================================
# JAEGAR:
# ----------------------------------------------------
JAEGAR_AGENT_HOST_NAME=172.17.120.23
JAEGER_AGENT_PORT=6831
# ----------------------------------------------------

# ====================================================
# SESSION:
# ----------------------------------------------------
JWT_TOKEN_LOCATION=headers
SECRET_KEY=super-secret-123

# ----------------------------------------------------
# CERTIFICATES AND KEYS:
# ----------------------------------------------------
CERTIFICATE_APP=tls/wildcard.cluster.local.crt
KEY_APP=tls/wildcard.cluster.local.key

# ----------------------------------------------------
# OAUTH2:
# ----------------------------------------------------
IAM_SERVER_URL=https://127.0.0.1:4000
TOKEN_URL=https://127.0.0.1:4000/oauth2/login
AUTH_URL=https://127.0.0.1:4000/oauth2/authorize

# ====================================================
# CORS:
# ----------------------------------------------------
CORS_ALLOWED_ORIGINS="[
    '*'
]"

# ====================================================
# LOGS LEVEL
# CRITICAL, ERROR, WARNING, WARN, INFO, DEBUG, NOTSET
# ----------------------------------------------------
LOGGING_LEVEL=DEBUG
```

### Usage

#### Running the Consumer

To run the consumer:

```code
python main.py
```

This will start the **worker**, which:
1. Will begin reading from the input topics, aggregating messages, processing
them, and then forwarding them to the output topics
2. will expose configured APIs from **endpoint.json** via **Swagger** over HTTPS. 

## Customization and Implementation

### Customizing `process()`

The `process()` function in `worker_kafka.py` is where you can implement your custom processing logic for ETL. This
function receives
the aggregated message and worker's metadatas and is expected to return the processed message that will be forwarded to
the output topics.

The `process()` function in `worker_api.py` is where you can implement your custom processing logic for API. This
function receives
the request and is expected to return the processed message that will be forwarded as response.

### Example Customization

#### Customizing `process()` to Call an External API

In many cases, you may want to enrich the message data by calling an external API. The `process()` function can be
customized to make such API calls, process the response, and append the result to the `content` of the message.

### Example Customization: Calling an External API in ETL pipeline

Here is an example of how you might modify the `process()` ETL function to call an external API and include the API
response in the `content` of the message:

```python
import requests
from utils.logger import logger


def process(message, consumer_name, metadatas):
    """
    Process the Kafka message by calling an external API and appending its result to the content.

    Args:
        message (dict): The aggregated message with fields combined from one or more topics.
        consumer_name (str): The name of the consumer, as defined in `config.py`. This is also the
                             ID of the consumer in the database and is used for logging purposes.
        metadatas (str): Additional metadata passed from the database configuration.

    Returns:
        dict: The final processed message, including the result from the external API.
    """

    # Example: Check if the 'text' field exists and call an external API with this text
    if "text" in message["content"]:
        try:
            # Example API call
            api_url = "https://api.example.com/analyze"
            response = requests.post(api_url, json={"text": message["content"]["text"]})

            if response.status_code == 200:
                api_result = response.json()

                # Assume the API returns a dictionary of results that you want to add to the content
                message["content"].update(api_result)

                logger.debug(f"{consumer_name}: API call successful, added {list(api_result.keys())} to content")
            else:
                logger.error(f"{consumer_name}: API call failed with status code {response.status_code}")
        except requests.exceptions.RequestException as e:
            logger.error(f"{consumer_name}: API call failed with exception {e}")

    return message
```

### Example Customization: Calling an External API in `worker_api.py`

Here is an example of how you might modify the `process()` API function to call an external API and include the API
response in the `content` of the message:

```python
import requests
from utils.logger import logger


def process(app, access_token=None, operation: Optional[str] = None, request=None):
    """
    Process the incoming REST request and return the response in JSON format.

    This function processes the request object (which is the model received from the REST endpoint),
    performs necessary operations or transformations, and returns the response in a JSON-serializable format.

    Args:
        app (object): The Flask or application object that handles the request lifecycle and context.
        access_token (str, optional): The access token for authentication, if required by the endpoint. Defaults to None.
        operation (str, optional): The specific operation to be performed, can be used to route logic within this function.
        request (object): The request object that contains the data sent in the REST API call. Typically, this will be a Flask request object.

    Returns:
        dict: A dictionary containing the processed data, ready to be returned as the JSON response body.

    Example Usage:
        - This function is called when a REST API endpoint is triggered, and it processes the incoming request,
          performs validation or transformation, and returns the output in a JSON format.

    Notes:
        - The request object is expected to contain the necessary payload or parameters sent by the client in the API request.
        - The `lower_req` method, which is invoked within this function, appears to handle some form of data normalization or transformation on the request.
    """
    mx = f'({inspect.currentframe().f_code.co_name})'
    message = lower_req(request, mx=mx)

    # Example: Check if the 'text' field exists and call an external API with this text
    if "text" in message["content"]:
        try:
            # Example API call
            api_url = "https://api.example.com/analyze"
            response = requests.post(api_url, json={"text": message["content"]["text"]})

            if response.status_code == 200:
                api_result = response.json()

                # Assume the API returns a dictionary of results that you want to add to the content
                message["content"].update(api_result)

                logger.debug(f"{consumer_name}: API call successful, added {list(api_result.keys())} to content")
            else:
                logger.error(f"{consumer_name}: API call failed with status code {response.status_code}")
        except requests.exceptions.RequestException as e:
            logger.error(f"{consumer_name}: API call failed with exception {e}")

    return message
```

Key Considerations When Calling an External API

1. **Error Handling**:
    - Ensure that you handle API errors gracefully. If the API call fails, decide whether to skip the message, retry the
      API call, or log the error and continue processing.
    - Use appropriate logging to capture both successes and failures of the API calls.
2. **Performance**
    - External API calls can introduce latency into the message processing pipeline. Consider the impact of this latency
      on your overall system performance and message throughput.
    - If the API response time is critical, you may want to implement asynchronous processing or use a message queue to
      handle API calls in parallel.
3. **Security**
    - Secure the API call by handling sensitive data appropriately (e.g., API keys, tokens).
    - Ensure that you’re making API calls over HTTPS to protect the data in transit.
4. **Idempotency**
    - Ensure that the `process()` function remains idempotent even when calling an external API. This means that if the
      same message is processed multiple times, the outcome should be the same.
    - If the API provides non-idempotent results (e.g., returning a timestamp), consider how you will handle these
      cases.

#### Example: Enriching a Message with Sentiment Analysis

- Suppose you have an external API that provides sentiment analysis based on text. Here’s how you might integrate it in
  ETL pipeline (`worker_kafka.py`):

```python
def process(message, consumer_name, metadatas):
    mx = f'({inspect.currentframe().f_code.co_name})'
    message = lower_req(request, mx=mx)
    if "text" in message["content"]:
        try:
            # Call the sentiment analysis API
            api_url = "https://api.example.com/sentiment"
            response = requests.post(api_url, json={"text": message["content"]["text"]})

            if response.status_code == 200:
                sentiment_result = response.json()

                # Add the sentiment analysis result to the content
                message["content"]["sentiment"] = sentiment_result.get("sentiment")

                logger.debug(f"{consumer_name}: Sentiment analysis added to message")
            else:
                logger.error(f"{consumer_name}: Sentiment analysis API call failed with status {response.status_code}")
        except requests.exceptions.RequestException as e:
            logger.error(f"{consumer_name}: Sentiment analysis API call failed with exception {e}")

    return message
```

In this example, the `process()` function calls an external sentiment analysis API, retrieves the sentiment score, and
appends it to the message content. The result is then returned and sent to the output Kafka topics.

- Suppose you have an external API that provides sentiment analysis based on text. Here’s how you might integrate it in
  on-demand API call (`worker_api.py`):

```python
def process(app, access_token=None, operation: Optional[str] = None, request=None):
    if "text" in message["content"]:
        try:
            # Call the sentiment analysis API
            api_url = "https://api.example.com/sentiment"
            response = requests.post(api_url, json={"text": message["content"]["text"]})

            if response.status_code == 200:
                sentiment_result = response.json()

                # Add the sentiment analysis result to the content
                message["content"]["sentiment"] = sentiment_result.get("sentiment")

                logger.debug(f"{consumer_name}: Sentiment analysis added to message")
            else:
                logger.error(f"{consumer_name}: Sentiment analysis API call failed with status {response.status_code}")
        except requests.exceptions.RequestException as e:
            logger.error(f"{consumer_name}: Sentiment analysis API call failed with exception {e}")

    return message
```

In this example, the `process()` function calls an external sentiment analysis API, retrieves the sentiment score, and
appends it to the message content. The result is then returned and sent as HTTP response.

- Edit `endpoint.json` to add and configure new endpoints for API. These will be served automatically over **HTTPS** and
  are configured to automatically be documented via **Swagger**.

```code
{
    "namespaces": [
        {
            "name": "api",
            "description": "api"
        }
    ],
    "models": {
        "model": {
            "id": {"type": "string", "args": {"required":true, "min_length":1, "description": "id", "example": "123"}},
            "message": {"type": "string", "args": {"required":true, "min_length":1, "description": "message", "example": "message"}}
        }
    },
    "endpoints": [
        {
            "operation_name": "input", "namespace": "api",
            "model_name": "model",
            "request_method": ["post"],
            "api_url": "/",
            "api_security": ["oauth2", "apikey"],
            "security_roles": null,
            "exec_method": {"module_name": "ops_service.worker_api", "method_name": "process"}
        }
    ]
}

```

**Namespaces**: logic separation of APIs (e.g. `/dev/`, `/api/`, `/v1/`)

**Models**: input models for requests. It supports `type` (_string, boolean, integer, dict_), params (_required, min_length, max_length, description, example_)

**Endpoints**:
1. configure **operation_name**: it is the endpoint's name
2. configure **namespace**: place the endpoint into a namespace
3. configure **request_method**: it can be "_post_", "_get_", "_put_", "_delete_"
4. configure **exec_method**: _module_name_ represents the path to service file where endpoint's service logic is implemented (e.g. _ops_service.worker_api_), _method_name_ represents the method the endpoint's service logic is implemented in  
5. **api_url**, **api_security**, **security_roles**: should remain the same as in the example. They can be configured to work with an Identity and Access Management (IAM) for auto-handling auth and roles if needed.

## Considerations

- **Timeouts**: The `MESSAGE_TIMEOUT` setting in `config.py` controls how long the consumer waits for all expected
  messages with the same ID to arrive before discarding the incomplete aggregation. Adjust this value based on the
  expected delay between messages from different topics.
- **Logging**: The project uses a centralized logging setup (via `logger.py`) to capture important events, errors, and
  debugging information. Make sure to configure the log level appropriately (**DEBUG**, **INFO**, **WARN**, etc.) for
  your deployment environment.
- **Database Configuration**: The consumer configuration is stored in a database, which allows for flexibility in
  changing the consumer behavior without modifying the code. Ensure that your database is secured and that access is
  appropriately controlled.

## Future Enhancements

- **Support for More Databases**: Expand the ORM support to include other databases such as PostgreSQL, SQLite, etc.,
  making the consumer skeleton more versatile.
- **Monitoring and Metrics**: Integrate monitoring tools (opentelemetry) for Kafka input messages (at the moment, it's
  already done for kafka output messages and API calls - see `tracing.py`)
- **Error Handling Enhancements**: Improve error handling mechanisms by adding support for retries, dead-letter queues (
  DLQs), and real-time alerting in case of failures or anomalies.
